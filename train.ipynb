{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/code/MMN\n",
      "Datasets\t eval_metrics.py      random_erasing.py  test.py\n",
      "README.md\t log\t\t      requirements.txt\t train.ipynb\n",
      "__pycache__\t loss.py\t      requirements.yml\t train.py\n",
      "data_loader.py\t model.py\t      resnet.py\t\t utils.py\n",
      "data_manager.py  pre_process_sysu.py  save_model\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Args:Namespace(dataset='sysu', lr=0.1, optim='sgd', arch='resnet50', resume='', test_only=False, model_path='save_model/', save_epoch=20, log_path='log/', vis_log_path='log/vis_log/', workers=7, img_w=192, img_h=384, batch_size=2, test_batch=64, method='agw', margin=0.3, num_pos=2, trial=1, seed=0, gpu='0', mode='all', delta=0.5)\n",
      "==========\n",
      "==> Loading data..\n",
      "Dataset sysu statistics:\n",
      "  ------------------------------\n",
      "  subset   | # ids | # images\n",
      "  ------------------------------\n",
      "  visible  |   395 |    22258\n",
      "  thermal  |   395 |    11909\n",
      "  ------------------------------\n",
      "  query    |    96 |     3803\n",
      "  gallery  |    96 |      301\n",
      "  ------------------------------\n",
      "Data Loading Time:\t 424.704\n",
      "==> Building model..\n",
      "==> Building model..\n",
      "==> Building model..\n",
      "==> Building model..\n",
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from data_loader import SYSUData, RegDBData, TestData\n",
    "from data_manager import *\n",
    "from eval_metrics import eval_sysu, eval_regdb\n",
    "from model import embed_net\n",
    "from utils import *\n",
    "from loss import OriTripletLoss, TriLoss, DCLoss\n",
    "from tensorboardX import SummaryWriter\n",
    "from random_erasing import RandomErasing\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Cross-Modality Training')\n",
    "parser.add_argument('--dataset', default='sysu', help='dataset name: regdb or sysu]')\n",
    "parser.add_argument('--lr', default=0.1 , type=float, help='learning rate, 0.00035 for adam')\n",
    "parser.add_argument('--optim', default='sgd', type=str, help='optimizer')\n",
    "parser.add_argument('--arch', default='resnet50', type=str, help='network baseline:resnet18 or resnet50')\n",
    "parser.add_argument('--resume', '-r', default='', type=str, help='resume from checkpoint')\n",
    "parser.add_argument('--test-only', action='store_true', help='test only')\n",
    "parser.add_argument('--model_path', default='save_model/', type=str, help='model save path')\n",
    "parser.add_argument('--save_epoch', default=20, type=int, metavar='s', help='save model every 10 epochs')\n",
    "parser.add_argument('--log_path', default='log/', type=str, help='log save path')\n",
    "parser.add_argument('--vis_log_path', default='log/vis_log/', type=str, help='log save path')\n",
    "parser.add_argument('--workers', default=7, type=int, metavar='N', help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--img_w', default=192, type=int, metavar='imgw', help='img width')\n",
    "parser.add_argument('--img_h', default=384, type=int, metavar='imgh', help='img height')\n",
    "parser.add_argument('--batch-size', default=2, type=int, metavar='B', help='training batch size')\n",
    "parser.add_argument('--test-batch', default=64, type=int, metavar='tb', help='testing batch size')\n",
    "parser.add_argument('--method', default='agw', type=str, metavar='m', help='method type: base or agw')\n",
    "parser.add_argument('--margin', default=0.3, type=float, metavar='margin', help='triplet loss margin')\n",
    "parser.add_argument('--num_pos', default=2, type=int, help='num of pos per identity in each modality')\n",
    "parser.add_argument('--trial', default=1, type=int, metavar='t', help='trial (only for RegDB dataset)')\n",
    "parser.add_argument('--seed', default=0, type=int, metavar='t', help='random seed')\n",
    "parser.add_argument('--gpu', default='0', type=str, help='gpu device ids for CUDA_VISIBLE_DEVICES')\n",
    "parser.add_argument('--mode', default='all', type=str, help='all or indoor')\n",
    "parser.add_argument('--delta', default=0.2, type=float, metavar='delta', help='dcl weights, 0.2 for PCB, 0.5 for resnet50')\n",
    "\n",
    "simulated_args = [\n",
    "    '--dataset', 'sysu',\n",
    "    '--lr', '0.1',\n",
    "    '--optim', 'sgd',\n",
    "    '--arch', 'resnet50',\n",
    "    '--resume', '',  \n",
    "    # '--test-only',  \n",
    "    '--model_path', 'save_model/',\n",
    "    '--save_epoch', '20',\n",
    "    '--log_path', 'log/',\n",
    "    '--vis_log_path', 'log/vis_log/',\n",
    "    '--workers', '7',\n",
    "    '--img_w', '192',\n",
    "    '--img_h', '384',\n",
    "    '--batch-size', '2',\n",
    "    '--test-batch', '64',\n",
    "    '--method', 'agw',\n",
    "    '--margin', '0.3',\n",
    "    '--num_pos', '2',\n",
    "    # '--trial', '1',\n",
    "    '--seed', '0',\n",
    "    '--gpu', '0',\n",
    "    '--mode', 'all',\n",
    "    '--delta', '0.5'\n",
    "]\n",
    "\n",
    "\n",
    "args = parser.parse_args(simulated_args)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "\n",
    "set_seed(args.seed)\n",
    "\n",
    "dataset = args.dataset\n",
    "if dataset == 'sysu':\n",
    "    data_path = './Datasets/SYSU-MM01/'\n",
    "    log_path = args.log_path + 'sysu_log/'\n",
    "    test_mode = [1, 2]  # thermal to visible\n",
    "elif dataset == 'regdb':\n",
    "    data_path = './Datasets/RegDB/'\n",
    "    log_path = args.log_path + 'regdb_log/'\n",
    "    test_mode = [2, 1]  # visible to thermal\n",
    "\n",
    "checkpoint_path = args.model_path\n",
    "\n",
    "if not os.path.isdir(log_path):\n",
    "    os.makedirs(log_path)\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "if not os.path.isdir(args.vis_log_path):\n",
    "    os.makedirs(args.vis_log_path)\n",
    "\n",
    "suffix = dataset\n",
    "if args.method=='agw':\n",
    "    suffix = suffix + '_agw_p{}_n{}_lr_{}_seed_{}'.format(args.num_pos, args.batch_size, args.lr, args.seed)\n",
    "else:\n",
    "    suffix = suffix + '_base_p{}_n{}_lr_{}_seed_{}'.format(args.num_pos, args.batch_size, args.lr, args.seed)\n",
    "\n",
    "\n",
    "if not args.optim == 'sgd':\n",
    "    suffix = suffix + '_' + args.optim\n",
    "\n",
    "if dataset == 'regdb':\n",
    "    suffix = suffix + '_trial_{}'.format(args.trial)\n",
    "\n",
    "sys.stdout = Logger(log_path + suffix + '_os.txt')\n",
    "\n",
    "vis_log_dir = args.vis_log_path + suffix + '/'\n",
    "\n",
    "if not os.path.isdir(vis_log_dir):\n",
    "    os.makedirs(vis_log_dir)\n",
    "writer = SummaryWriter(vis_log_dir)\n",
    "print(\"==========\\nArgs:{}\\n==========\".format(args))\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0\n",
    "\n",
    "print('==> Loading data..')\n",
    "# Data loading code\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Pad(10),\n",
    "    transforms.RandomCrop((args.img_h, args.img_w)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "    RandomErasing(probability = 0.5, mean=[0.0, 0.0, 0.0]),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((args.img_h, args.img_w)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "end = time.time()\n",
    "if dataset == 'sysu':\n",
    "    # training set\n",
    "    trainset = SYSUData(data_path, transform=transform_train)\n",
    "    # generate the idx of each person identity\n",
    "    color_pos, thermal_pos = GenIdx(trainset.train_color_label, trainset.train_thermal_label)\n",
    "\n",
    "    # testing set\n",
    "    query_img, query_label, query_cam = process_query_sysu(data_path, mode=args.mode)\n",
    "    gall_img, gall_label, gall_cam = process_gallery_sysu(data_path, mode=args.mode, trial=0)\n",
    "\n",
    "elif dataset == 'regdb':\n",
    "    # training set\n",
    "    trainset = RegDBData(data_path, args.trial, transform=transform_train)\n",
    "    # generate the idx of each person identity\n",
    "    color_pos, thermal_pos = GenIdx(trainset.train_color_label, trainset.train_thermal_label)\n",
    "\n",
    "    # testing set\n",
    "    query_img, query_label = process_test_regdb(data_path, trial=args.trial, modal='visible')\n",
    "    gall_img, gall_label = process_test_regdb(data_path, trial=args.trial, modal='thermal')\n",
    "\n",
    "gallset = TestData(gall_img, gall_label, transform=transform_test, img_size=(args.img_w, args.img_h))\n",
    "queryset = TestData(query_img, query_label, transform=transform_test, img_size=(args.img_w, args.img_h))\n",
    "\n",
    "# testing data loader\n",
    "gall_loader = data.DataLoader(gallset, batch_size=args.test_batch, shuffle=False, num_workers=args.workers)\n",
    "query_loader = data.DataLoader(queryset, batch_size=args.test_batch, shuffle=False, num_workers=args.workers)\n",
    "\n",
    "n_class = len(np.unique(trainset.train_color_label))\n",
    "nquery = len(query_label)\n",
    "ngall = len(gall_label)\n",
    "\n",
    "print('Dataset {} statistics:'.format(dataset))\n",
    "print('  ------------------------------')\n",
    "print('  subset   | # ids | # images')\n",
    "print('  ------------------------------')\n",
    "print('  visible  | {:5d} | {:8d}'.format(n_class, len(trainset.train_color_label)))\n",
    "print('  thermal  | {:5d} | {:8d}'.format(n_class, len(trainset.train_thermal_label)))\n",
    "print('  ------------------------------')\n",
    "print('  query    | {:5d} | {:8d}'.format(len(np.unique(query_label)), nquery))\n",
    "print('  gallery  | {:5d} | {:8d}'.format(len(np.unique(gall_label)), ngall))\n",
    "print('  ------------------------------')\n",
    "print('Data Loading Time:\\t {:.3f}'.format(time.time() - end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1472/417022284.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_local\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'off'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgm_pool\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m'off'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_local\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'on'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgm_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'on'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/MMN/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, class_num, no_local, gm_pool, arch)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisible_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'visible'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthermal_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'thermal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;31m# self.cbam = CBAM(in_channel=3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_resnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/MMN/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channel, reduction, kernel_size)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_channel\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_channel\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         )\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch1.8/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch1.8/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch1.8/lib/python3.9/site-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0mfan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_calculate_correct_fan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0mgain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonlinearity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;31m# Calculate uniform bounds from standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "print('==> Building model..')\n",
    "if args.method =='base':\n",
    "    net = embed_net(n_class, no_local= 'off', gm_pool =  'off', arch=args.arch)\n",
    "else:\n",
    "    net = embed_net(n_class, no_local= 'on', gm_pool = 'on', arch=args.arch)\n",
    "net.to(device)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if len(args.resume) > 0:\n",
    "    model_path = checkpoint_path + args.resume\n",
    "    if os.path.isfile(model_path):\n",
    "        print('==> loading checkpoint {}'.format(args.resume))\n",
    "        checkpoint = torch.load(model_path)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        print('==> loaded checkpoint {} (epoch {})'\n",
    "              .format(args.resume, checkpoint['epoch']))\n",
    "    else:\n",
    "        print('==> no checkpoint found at {}'.format(args.resume))\n",
    "\n",
    "# define loss function\n",
    "criterion_id = nn.CrossEntropyLoss()\n",
    "\n",
    "loader_batch = args.batch_size * args.num_pos\n",
    "criterion_tri= OriTripletLoss(batch_size=loader_batch, margin=args.margin)\n",
    "self_critial= TriLoss(batch_size=loader_batch, margin=args.margin)\n",
    "criterion_div = DCLoss(num=2)\n",
    "\n",
    "criterion_id.to(device)\n",
    "criterion_tri.to(device)\n",
    "criterion_div.to(device)\n",
    "\n",
    "if args.optim == 'sgd':\n",
    "    # 生成所有bottlenecks和classifiers的参数列表\n",
    "    ignored_params = list(map(id, chain(*[b.parameters() for b in net.bottlenecks]))) \\\n",
    "                   + list(map(id, chain(*[c.parameters() for c in net.classifiers])))\n",
    "\n",
    "    # 通过过滤的方式，排除上述特定层的参数，获取基础参数\n",
    "    base_params = filter(lambda p: id(p) not in ignored_params, net.parameters())\n",
    "\n",
    "    # 定义优化器，为不同的参数组设置不同的学习率\n",
    "    optimizer = optim.SGD([\n",
    "        {'params': base_params, 'lr': 0.1 * args.lr},  # 基础参数，学习率较低\n",
    "        *[\n",
    "            {'params': b.parameters(), 'lr': args.lr} for b in net.bottlenecks\n",
    "        ],\n",
    "        *[\n",
    "            {'params': c.parameters(), 'lr': args.lr} for c in net.classifiers\n",
    "        ]\n",
    "    ], weight_decay=5e-4, momentum=0.9, nesterov=True)\n",
    "\n",
    "# if args.optim == 'sgd':\n",
    "#     ignored_params = list(map(id, net.bottleneck1.parameters())) \\\n",
    "#                      + list(map(id, net.bottleneck2.parameters())) \\\n",
    "#                      + list(map(id, net.bottleneck3.parameters())) \\\n",
    "#                      + list(map(id, net.bottleneck4.parameters())) \\\n",
    "#                      + list(map(id, net.classifier1.parameters())) \\\n",
    "#                      + list(map(id, net.classifier2.parameters())) \\\n",
    "#                      + list(map(id, net.classifier3.parameters())) \\\n",
    "#                      + list(map(id, net.classifier4.parameters()))\n",
    "\n",
    "#     base_params = filter(lambda p: id(p) not in ignored_params, net.parameters())\n",
    "\n",
    "#     optimizer = optim.SGD([\n",
    "#         {'params': base_params, 'lr': 0.1 * args.lr},\n",
    "#         {'params': net.bottleneck1.parameters(), 'lr': args.lr},\n",
    "#         {'params': net.bottleneck2.parameters(), 'lr': args.lr},\n",
    "#         {'params': net.bottleneck3.parameters(), 'lr': args.lr},\n",
    "#         {'params': net.bottleneck4.parameters(), 'lr': args.lr},\n",
    "#         {'params': net.classifier1.parameters(), 'lr': args.lr},\n",
    "#         {'params': net.classifier2.parameters(), 'lr': args.lr},\n",
    "#         {'params': net.classifier3.parameters(), 'lr': args.lr},\n",
    "#         {'params': net.classifier4.parameters(), 'lr': args.lr}],\n",
    "#         weight_decay=5e-4, momentum=0.9, nesterov=True)\n",
    "    \n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    if epoch < 10:\n",
    "        lr = args.lr * (epoch + 1) / 10\n",
    "    elif epoch >= 10 and epoch < 20:\n",
    "        lr = args.lr\n",
    "    elif epoch >= 20 and epoch < 50:\n",
    "        lr = args.lr * 0.1\n",
    "    elif epoch >= 50:\n",
    "        lr = args.lr * 0.01\n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = 0.1 * lr\n",
    "    for i in range(len(optimizer.param_groups) - 1):\n",
    "        optimizer.param_groups[i + 1]['lr'] = lr\n",
    "\n",
    "    return lr\n",
    "\n",
    "def train(epoch):\n",
    "\n",
    "    current_lr = adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss = AverageMeter()\n",
    "    id_loss = AverageMeter()\n",
    "    tri_loss = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # switch to train mode\n",
    "    net.train()\n",
    "    end = time.time()\n",
    "\n",
    "    for batch_idx, (input1, input2, label1, label2) in enumerate(trainloader):\n",
    "\n",
    "        labels = torch.cat((label1, label1, label2, label2), 0)\n",
    "\n",
    "        input1 = Variable(input1.cuda())\n",
    "        input2 = Variable(input2.cuda())\n",
    "\n",
    "        labels = Variable(labels.cuda())\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "\n",
    "        feat1, feat2, feat3, feat4, out1, out2, out3, out4, g1 = net(input1, input2)\n",
    "\n",
    "        loss_id = (criterion_id(out1, labels) + criterion_id(out2, labels) + criterion_id(out3, labels) + criterion_id(out4, labels))*0.25\n",
    "        \n",
    "        lbs = torch.cat((label1, label2), 0)\n",
    "\n",
    "        ft11, ft12, ft13, ft14 = torch.chunk(feat1, 4, 0)\n",
    "        ft21, ft22, ft23, ft24 = torch.chunk(feat2, 4, 0)\n",
    "        ft31, ft32, ft33, ft34 = torch.chunk(feat3, 4, 0)\n",
    "        ft41, ft42, ft43, ft44 = torch.chunk(feat4, 4, 0)\n",
    "        ba= criterion_tri(ft11, label1)[1]\n",
    "\n",
    "        loss_tri1 = (criterion_tri(torch.cat((ft11, ft13),0), lbs)[0] + criterion_tri(torch.cat((ft11, ft14),0), lbs)[0] + criterion_tri(torch.cat((ft12, ft13),0), lbs)[0] + criterion_tri(torch.cat((ft12, ft14),0), lbs)[0])/4\n",
    "        loss_tri2 = (criterion_tri(torch.cat((ft21, ft23),0), lbs)[0] + criterion_tri(torch.cat((ft21, ft24),0), lbs)[0] + criterion_tri(torch.cat((ft22, ft23),0), lbs)[0] + criterion_tri(torch.cat((ft22, ft24),0), lbs)[0])/4\n",
    "        loss_tri3 = (criterion_tri(torch.cat((ft31, ft33),0), lbs)[0] + criterion_tri(torch.cat((ft31, ft34),0), lbs)[0] + criterion_tri(torch.cat((ft32, ft33),0), lbs)[0] + criterion_tri(torch.cat((ft32, ft34),0), lbs)[0])/4\n",
    "        loss_tri4 = (criterion_tri(torch.cat((ft41, ft43),0), lbs)[0] + criterion_tri(torch.cat((ft41, ft44),0), lbs)[0] + criterion_tri(torch.cat((ft42, ft43),0), lbs)[0] + criterion_tri(torch.cat((ft42, ft44),0), lbs)[0])/4\n",
    "        \n",
    "        loss_tri = (loss_tri1 + loss_tri2 + loss_tri3 + loss_tri4)/4\n",
    "        \n",
    "        loss_dcl = (criterion_div(torch.cat((ft12, ft14),0)) + criterion_div(torch.cat((ft22, ft24),0)) + criterion_div(torch.cat((ft32, ft34),0)) + criterion_div(torch.cat((ft42, ft44),0)))*0.25*args.delta\n",
    "\n",
    "\n",
    "        correct += (ba / 2)\n",
    "        _, predicted = out1.max(1)\n",
    "        correct += (predicted.eq(labels).sum().item() / 2)\n",
    "\n",
    "        loss = loss_id + loss_tri + loss_dcl\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update P\n",
    "        train_loss.update(loss.item(), 2 * input1.size(0))\n",
    "        id_loss.update(loss_id.item(), 2 * input1.size(0))\n",
    "        tri_loss.update(loss_tri.item(), 2 * input1.size(0))\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Epoch: [{}][{}/{}] '\n",
    "                  'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n",
    "                  'lr:{:.3f} '\n",
    "                  'Loss: {train_loss.val:.4f} ({train_loss.avg:.4f}) '\n",
    "                  'iLoss: {id_loss.val:.4f} ({id_loss.avg:.4f}) '\n",
    "                  'TLoss: {tri_loss.val:.4f} ({tri_loss.avg:.4f}) '\n",
    "                  'Accu: {:.2f}'.format(\n",
    "                epoch, batch_idx, len(trainloader), current_lr,\n",
    "                100. * correct / total, batch_time=batch_time,\n",
    "                train_loss=train_loss, id_loss=id_loss, tri_loss=tri_loss))\n",
    "\n",
    "    writer.add_scalar('total_loss', train_loss.avg, epoch)\n",
    "    writer.add_scalar('id_loss', id_loss.avg, epoch)\n",
    "    writer.add_scalar('tri_loss', tri_loss.avg, epoch)\n",
    "    writer.add_scalar('lr', current_lr, epoch)\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    # switch to evaluation mode\n",
    "    net.eval()\n",
    "    print('Extracting Gallery Feature...')\n",
    "    start = time.time()\n",
    "    ptr = 0\n",
    "    gall_feat = np.zeros((ngall, 2048*4))\n",
    "    gall_feat_att = np.zeros((ngall, 2048*4))\n",
    "    Xgall_feat = np.zeros((ngall, 2048*4))\n",
    "    Xgall_feat_att = np.zeros((ngall, 2048*4))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, label) in enumerate(gall_loader):\n",
    "            batch_num = input.size(0)\n",
    "            input = Variable(input.cuda())\n",
    "            feat, feat_att = net(input, input, test_mode[0])\n",
    "            gall_feat[ptr:ptr + batch_num, :] = feat[:batch_num].detach().cpu().numpy()\n",
    "            gall_feat_att[ptr:ptr + batch_num, :] = feat_att[:batch_num].detach().cpu().numpy()\n",
    "            Xgall_feat[ptr:ptr + batch_num, :] = feat[batch_num:].detach().cpu().numpy()\n",
    "            Xgall_feat_att[ptr:ptr + batch_num, :] = feat_att[batch_num:].detach().cpu().numpy()\n",
    "            ptr = ptr + batch_num\n",
    "    print('Extracting Time:\\t {:.3f}'.format(time.time() - start))\n",
    "\n",
    "    # switch to evaluation\n",
    "    net.eval()\n",
    "    print('Extracting Query Feature...')\n",
    "    start = time.time()\n",
    "    ptr = 0\n",
    "    query_feat = np.zeros((nquery, 2048*4))\n",
    "    query_feat_att = np.zeros((nquery, 2048*4))\n",
    "    Xquery_feat = np.zeros((nquery, 2048*4))\n",
    "    Xquery_feat_att = np.zeros((nquery, 2048*4))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, label) in enumerate(query_loader):\n",
    "            batch_num = input.size(0)\n",
    "            input = Variable(input.cuda())\n",
    "            feat, feat_att = net(input, input, test_mode[1])\n",
    "            query_feat[ptr:ptr + batch_num, :] = feat[:batch_num].detach().cpu().numpy()\n",
    "            query_feat_att[ptr:ptr + batch_num, :] = feat_att[:batch_num].detach().cpu().numpy()\n",
    "            Xquery_feat[ptr:ptr + batch_num, :] = feat[batch_num:].detach().cpu().numpy()\n",
    "            Xquery_feat_att[ptr:ptr + batch_num, :] = feat_att[batch_num:].detach().cpu().numpy()\n",
    "            ptr = ptr + batch_num\n",
    "    print('Extracting Time:\\t {:.3f}'.format(time.time() - start))\n",
    "\n",
    "    start = time.time()\n",
    "    # compute the similarity\n",
    "    distmat = np.matmul(query_feat, np.transpose(gall_feat))\n",
    "    distmat_att = np.matmul(query_feat_att, np.transpose(gall_feat_att))\n",
    "    \n",
    "    Xdistmat = np.matmul(query_feat, np.transpose(Xgall_feat))\n",
    "    Xdistmat_att = np.matmul(query_feat_att, np.transpose(Xgall_feat_att))\n",
    "    \n",
    "    distmatX = np.matmul(Xquery_feat, np.transpose(gall_feat))\n",
    "    distmat_attX = np.matmul(Xquery_feat_att, np.transpose(gall_feat_att))\n",
    "    \n",
    "    XXdistmat = np.matmul(Xquery_feat, np.transpose(Xgall_feat))\n",
    "    XXdistmat_att = np.matmul(Xquery_feat_att, np.transpose(Xgall_feat_att))\n",
    "    # evaluation\n",
    "\n",
    "    cmc, mAP, mINP = eval_regdb(-distmat, query_label, gall_label)\n",
    "    cmc_att, mAP_att, mINP_att = eval_regdb(-distmat_att, query_label, gall_label)\n",
    "\n",
    "    Xcmc, XmAP, XmINP = eval_regdb(-Xdistmat, query_label, gall_label)\n",
    "    Xcmc_att, XmAP_att, XmINP_att = eval_regdb(-Xdistmat_att, query_label, gall_label)\n",
    "\n",
    "    cmcX, mAPX, mINPX = eval_regdb(-distmatX, query_label, gall_label)\n",
    "    cmc_attX, mAP_attX, mINP_attX = eval_regdb(-distmat_attX, query_label, gall_label)\n",
    "\n",
    "    XXcmc, XXmAP, XXmINP = eval_regdb(-XXdistmat, query_label, gall_label)\n",
    "    XXcmc_att, XXmAP_att, XXmINP_att = eval_regdb(-XXdistmat_att, query_label, gall_label)\n",
    "    print('Evaluation Time:\\t {:.3f}'.format(time.time() - start))\n",
    "\n",
    "    return cmc, mAP, mINP, cmc_att, mAP_att, mINP_att, \\\n",
    "    Xcmc, XmAP, XmINP, Xcmc_att, XmAP_att, XmINP_att, \\\n",
    "    cmcX, mAPX, mINPX, cmc_attX, mAP_attX, mINP_attX, \\\n",
    "    XXcmc, XXmAP, XXmINP, XXcmc_att, XXmAP_att, XXmINP_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Start Training...\n",
      "==> Preparing Data Loader...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36088/508729676.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'==> Preparing Data Loader...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# identity sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     sampler = IdentitySampler(trainset.train_color_label, \\\n\u001b[0m\u001b[1;32m      9\u001b[0m                               \u001b[0mtrainset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_thermal_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthermal_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                               epoch)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# training\n",
    "print('==> Start Training...')\n",
    "start_epoch = 0\n",
    "for epoch in range(start_epoch, 81 - start_epoch):\n",
    "\n",
    "    print('==> Preparing Data Loader...')\n",
    "    # identity sampler\n",
    "    sampler = IdentitySampler(trainset.train_color_label, \\\n",
    "                              trainset.train_thermal_label, color_pos, thermal_pos, args.num_pos, args.batch_size,\n",
    "                              epoch)\n",
    "\n",
    "    trainset.cIndex = sampler.index1  # color index\n",
    "    trainset.tIndex = sampler.index2  # thermal index\n",
    "    print(epoch)\n",
    "    print(trainset.cIndex)\n",
    "    print(trainset.tIndex)\n",
    "\n",
    "    loader_batch = args.batch_size * args.num_pos\n",
    "\n",
    "    #pin_memory=True\n",
    "    trainloader = data.DataLoader(trainset, batch_size=loader_batch, \\\n",
    "                                  sampler=sampler, num_workers=args.workers, drop_last=True , pin_memory=True)\n",
    "\n",
    "    # training\n",
    "    train(epoch)\n",
    "\n",
    "    if epoch > 0 and epoch % 2 == 0:\n",
    "        print('Test Epoch: {}'.format(epoch))\n",
    "    \n",
    "        # testing\n",
    "        cmc, mAP, mINP, cmc_att, mAP_att, mINP_att, \\\n",
    "        Xcmc, XmAP, XmINP, Xcmc_att, XmAP_att, XmINP_att, \\\n",
    "        cmcX, mAPX, mINPX, cmc_attX, mAP_attX, mINP_attX, \\\n",
    "        XXcmc, XXmAP, XXmINP, XXcmc_att, XXmAP_att, XXmINP_att = test(epoch)\n",
    "        # save model\n",
    "        if cmc_att[0] > best_acc:  # not the real best for sysu-mm01\n",
    "            best_acc = cmc_att[0]\n",
    "            best_epoch = epoch\n",
    "            state = {\n",
    "                'net': net.state_dict(),\n",
    "                'cmc': cmc_att,\n",
    "                'mAP': mAP_att,\n",
    "                'mINP': mINP_att,\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "            torch.save(state, checkpoint_path + suffix + '_best.t')\n",
    "    \n",
    "        # save model\n",
    "        if epoch > 10 and epoch % args.save_epoch == 0:\n",
    "            state = {\n",
    "                'net': net.state_dict(),\n",
    "                'cmc': cmc,\n",
    "                'mAP': mAP,\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "            torch.save(state, checkpoint_path + suffix + '_epoch_{}.t'.format(epoch))\n",
    "    \n",
    "        print('POOL:   Rank-1: {:.2%} | Rank-5: {:.2%} | Rank-10: {:.2%}| Rank-20: {:.2%}| mAP: {:.2%}| mINP: {:.2%}'.format(\n",
    "            cmc[0], cmc[4], cmc[9], cmc[19], mAP, mINP))\n",
    "        print('FC:   Rank-1: {:.2%} | Rank-5: {:.2%} | Rank-10: {:.2%}| Rank-20: {:.2%}| mAP: {:.2%}| mINP: {:.2%}'.format(\n",
    "            cmc_att[0], cmc_att[4], cmc_att[9], cmc_att[19], mAP_att, mINP_att))\n",
    "        print('POOL:   Rank-1: {:.2%} | Rank-5: {:.2%} | Rank-10: {:.2%}| Rank-20: {:.2%}| mAP: {:.2%}| mINP: {:.2%}'.format(\n",
    "            Xcmc[0], Xcmc[4], Xcmc[9],Xcmc[19], XmAP, XmINP))\n",
    "        print('FC:   Rank-1: {:.2%} | Rank-5: {:.2%} | Rank-10: {:.2%}| Rank-20: {:.2%}| mAP: {:.2%}| mINP: {:.2%}'.format(\n",
    "            Xcmc_att[0], Xcmc_att[4], Xcmc_att[9], Xcmc_att[19], XmAP_att, XmINP_att))\n",
    "        print('POOL:   Rank-1: {:.2%} | Rank-5: {:.2%} | Rank-10: {:.2%}| Rank-20: {:.2%}| mAP: {:.2%}| mINP: {:.2%}'.format(\n",
    "            cmcX[0], cmcX[4], cmcX[9], cmcX[19], mAPX, mINPX))\n",
    "        print('FC:   Rank-1: {:.2%} | Rank-5: {:.2%} | Rank-10: {:.2%}| Rank-20: {:.2%}| mAP: {:.2%}| mINP: {:.2%}'.format(\n",
    "            cmc_attX[0], cmc_attX[4], cmc_attX[9], cmc_attX[19], mAP_attX, mINP_attX))\n",
    "        print('POOL:   Rank-1: {:.2%} | Rank-5: {:.2%} | Rank-10: {:.2%}| Rank-20: {:.2%}| mAP: {:.2%}| mINP: {:.2%}'.format(\n",
    "            XXcmc[0], XXcmc[4], XXcmc[9], XXcmc[19], XXmAP, XXmINP))\n",
    "        print('FC:   Rank-1: {:.2%} | Rank-5: {:.2%} | Rank-10: {:.2%}| Rank-20: {:.2%}| mAP: {:.2%}| mINP: {:.2%}'.format(\n",
    "            XXcmc_att[0], XXcmc_att[4], XXcmc_att[9], XXcmc_att[19], XXmAP_att, XXmINP_att))\n",
    "        print('Best Epoch [{}]'.format(best_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/code/MMN\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets\t eval_metrics.py      random_erasing.py  test.py\n",
      "README.md\t log\t\t      requirements.txt\t train.ipynb\n",
      "__pycache__\t loss.py\t      requirements.yml\t train.py\n",
      "data_loader.py\t model.py\t      resnet.py\t\t utils.py\n",
      "data_manager.py  pre_process_sysu.py  save_model\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.8",
   "language": "python",
   "name": "torch1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
